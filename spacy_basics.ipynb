{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03a23f5c-0334-42cc-862b-2242d53f7c2e",
   "metadata": {},
   "source": [
    "# NLP Processing With spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e121ec-37e1-49af-a58b-5f976be6468b",
   "metadata": {},
   "source": [
    "SpaCy is a Python software library for automatic processing of many languages. It is an essential toolbox for the computational analysis of text corpora.\n",
    "\n",
    "This course is an introduction to its main features, including:\n",
    "\n",
    "- tokenization\n",
    "- lemmatization\n",
    "- named-Entities recognition and linking\n",
    "\n",
    "Some reading material:\n",
    "\n",
    "- https://spacy.io/\n",
    "- Avanced NLP with Spacy : https://course.spacy.io/en/\n",
    "- https://github.com/mchesterkadwell/named-entity-recognition/blob/main/1-basic-text-mining-concepts.ipynb\n",
    "- for NER : https://melaniewalsh.github.io/Intro-Cultural-Analytics/05-Text-Analysis/12-Named-Entity-Recognition.html\n",
    "- for Text-Mining (basics) : https://github.com/mchesterkadwell/named-entity-recognition/blob/main/1-basic-text-mining-concepts.ipynb \n",
    "\n",
    "This course includes parts of the excellent tutorial \"Natural Language Processing With spaCy in Python\": \n",
    "https://realpython.com/natural-language-processing-spacy-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfb6e30-1622-4dcd-9380-d78038d72ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install and import Spacy\n",
    "#!pip install -U spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb060bc-ccea-4fbc-80a1-9d5b0d6c02f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc0298e-5779-4e27-a8f1-caeede59afc1",
   "metadata": {},
   "source": [
    "## Language Processing Pipelines & Trained Models\n",
    "\n",
    "[https://spacy.io/usage/processing-pipelines](https://spacy.io/usage/processing-pipelines)\n",
    "\n",
    "When you call nlp on a text, spaCy first tokenizes the text to produce a [Doc object](https://spacy.io/api/doc). The Doc is then processed in several different steps (ie the processing pipeline). Each pipeline component returns the processed Doc, which is then passed on to the next component.\n",
    "\n",
    "![spacy_pipeline](img/spacy_pipeline.svg)\n",
    "\n",
    "The capabilities of a processing pipeline always depend on the components, their models and how they were trained. For example, a pipeline for named entity recognition needs to include a trained named entity recognizer component.\n",
    "\n",
    "Reference should be made to the documentation of the models made available : [https://spacy.io/models](https://spacy.io/models)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f815c4aa-5bed-4c1e-ba14-11cc5a09892d",
   "metadata": {},
   "source": [
    "**Models available for French:**\n",
    "\n",
    "|name|genre|size|use|components|\n",
    "|----|-----|----|---|----------|\n",
    "|[fr_core_news_sm](https://spacy.io/models/fr#fr_core_news_sm)|written text (news, media)|15 MB|CPU|tok2vec, morphologizer, parser, senter, attribute_ruler, lemmatizer, ner|\n",
    "|[fr_core_news_md](https://spacy.io/models/fr#fr_core_news_md)|written text (news, media)|43 MB|CPU|tok2vec, morphologizer, parser, senter, ner, attribute_ruler, lemmatizer|\n",
    "|[fr_core_news_lg](https://spacy.io/models/fr#fr_core_news_lg)|written text (news, media)|545 MN|CPU|tok2vec, morphologizer, parser, senter, ner, attribute_ruler, lemmatizer|\n",
    "|[fr_dep_news_trf](https://spacy.io/models/fr#fr_dep_news_trf)|written text (news, media)|382 MB|GPU (camembert-base)|transformer, morphologizer, parser, attribute_ruler, lemmatizer|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234790ee-785f-4a01-b709-3ae6f361f546",
   "metadata": {},
   "source": [
    "## Install and import trained pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c60048d-03f5-4fc4-a809-fc9b2523e3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda install -c conda-forge spacy-model-fr_core_news_md"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbce0521-c337-4095-b422-540ae574a537",
   "metadata": {},
   "source": [
    "The load() function returns a [Language callable object](https://spacy.io/api/language), which is commonly assigned to a variable called nlp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466a069d-1228-4115-86da-8660833ef152",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fr_core_news_md\n",
    "nlp = fr_core_news_md.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d1d546-ca7d-49e4-898d-afe62477126a",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9808e717-44f1-4145-8900-2807248e7ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849308ff-ba9f-41f9-8811-eb7b90d9fc5d",
   "metadata": {},
   "source": [
    "## Tokens\n",
    "\n",
    "To start processing your input, you construct a [Doc object](https://spacy.io/api/doc). \n",
    "\n",
    "A Doc object is a sequence of [Token](https://spacy.io/api/token) objects representing a lexical token (~ a word). each token contains different features describing it (lemma, morpho-syntactic label, etc.).\n",
    "\n",
    "**You can instantiate a Doc object by calling the Language object with the input string as an argument**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96891465-d375-44fc-8c10-d57ada81e20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "quote = 'Quant aux gens que j’accuse, je ne les connais pas, je ne les ai jamais vus, je n’ai contre eux ni rancune ni haine. Ils ne sont pour moi que des entités, des esprits de malfaisance sociale. Et l’acte que j’accomplis ici n’est qu’un moyen révolutionnaire pour hâter l’explosion de la vérité et de la justice.\\nJe n’ai qu’une passion, celle de la lumière, au nom de l’humanité qui a tant souffert et qui a droit au bonheur. Ma protestation enflammée n’est que le cri de mon âme. Qu’on ose donc me traduire en cour d’assises et que l’enquête ait lieu au grand jour !\\nJ’attends.'\n",
    "document = nlp(quote)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5853ff2-5773-4f82-856f-2e96fee5082e",
   "metadata": {},
   "source": [
    "Chaque token a de nombreux [attributs](https://spacy.io/api/token#attributes) facilement accessibles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77986de7-47b7-4f4e-8450-d770b9f40c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_tokens = []\n",
    "for token in document:\n",
    "    document_tokens.append(token.text)\n",
    "print(document_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f91f91-1a82-4d0a-bbf2-865f0a18f7f8",
   "metadata": {},
   "source": [
    "Or, more simply, by using a [List comprehension](https://www.w3schools.com/python/python_lists_comprehension.asp):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fc1a10-ba0c-4af8-989c-5cfc66fd251c",
   "metadata": {},
   "outputs": [],
   "source": [
    "[token.text for token in document]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc802d23-2f34-42e1-91c3-1d4d69243d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "for token in document:\n",
    "    print(token.text, token.lemma_, token.pos_, token.is_punct, token.is_stop, token.sent[:4])\n",
    "'''\n",
    "print(\n",
    "    f\"{'Index':9}\"\n",
    "    f\"{'Text':15}\"\n",
    "    f\"{'Lemma':15}\"\n",
    "    f\"{'POS':10}\"\n",
    "    f\"{'Punct?':10}\"\n",
    "    f\"{'Stop Word?':15}\"\n",
    "    f\"{'Sentence beginning'}\"\n",
    ")\n",
    "for token in document[71:92]:\n",
    "    print(\n",
    "        f\"{str(token.i):9}\"\n",
    "        f\"{str(token.text):15}\"\n",
    "        f\"{str(token.lemma_):15}\"\n",
    "        f\"{str(token.pos_):10}\"\n",
    "        f\"{str(token.is_punct):10}\"\n",
    "        f\"{str(token.is_stop):15}\"\n",
    "        f\"{str(token.sent[:6])+'…'}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8fda75-8d8d-4173-9c47-b59f211869e3",
   "metadata": {},
   "source": [
    "You may need to store each token and its attributes in a dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e20d89-959c-472e-a621-089ccc732613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# method 2: features 2 df\n",
    "import pandas as pd\n",
    "token_atts = []\n",
    "for token in document:\n",
    "    token_atts.append(\n",
    "        [token.text, token.lemma_, token.pos_, token.is_punct, token.is_stop, f\"{str(token.sent[:6])+'…'}\"]\n",
    "    )\n",
    "token_atts_df = pd.DataFrame(token_atts)\n",
    "token_atts_df.columns = ['Text', 'Lemma', 'POS', 'Is_Punct', 'Is_Stop_Word', 'Sentence_Begin']\n",
    "token_atts_df.iloc[71:92]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1f8251-4975-41f0-8679-2af06f6a3eff",
   "metadata": {},
   "source": [
    "You can customize the tokenizer by defining your own segmentation rules:\n",
    "\n",
    "- https://spacy.io/usage/linguistic-features#native-tokenizers\n",
    "- https://realpython.com/natural-language-processing-spacy-python/#tokens-in-spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c87d774-1d9f-4d5d-b490-d7aa69317a5a",
   "metadata": {},
   "source": [
    "More often than not, we need to load the textual content of a file to instantiate a Doc object.  \n",
    "The [pathlib](https://docs.python.org/3/library/pathlib.html) module offers classes representing filesystem paths with semantics appropriate for different operating systems.\n",
    "\n",
    "Let's try, to load Zola's famous text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ede27eb-e0fa-4142-ac31-bdf6cb5c73a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "file_path = './data/zola_accuse_fr.txt'\n",
    "zola_doc = nlp(pathlib.Path(file_path).read_text(encoding=\"utf-8\"))\n",
    "print([token.text for token in zola_doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814f3f5c-f949-448b-90ce-45b0fe3c8594",
   "metadata": {},
   "source": [
    "## Sentence Detection\n",
    "\n",
    "The following examples are taken from Real Python introduction: https://realpython.com/natural-language-processing-spacy-python/\n",
    "\n",
    "Sentence detection is the process of locating where sentences start and end in a given text. This allows you to you divide a text into linguistically meaningful units.\n",
    "\n",
    "In spaCy, the `.sents` property is used to extract sentences from the Doc object. Here’s how you would extract the total number of sentences and the sentences themselves for a given input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aad3e6d-feba-4583-8310-6c4220abb819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# counting sentences\n",
    "sentences = list(zola_doc.sents)\n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971171cd-29c2-412b-b688-0dde9a7f893d",
   "metadata": {},
   "source": [
    "There's no built-in sentence index: you need to iterate over sentences. Or you can use the [list()](https://www.w3schools.com/python/ref_func_list.asp) function to creates a list object. The list method `len()` returns the number of elements in this list. This also provides an index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35d3c59-cb43-49de-8681-4d4d59f79fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the 10th sentence\n",
    "sentences[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2617f2-2883-47c5-a5ce-1a9e5790aec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display all sentences\n",
    "i=0\n",
    "for sentence in sentences:\n",
    "    print(f'{i}: {sentence[:10]}…')\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cab4a7d-d0b7-415c-b350-dcc10ba85d50",
   "metadata": {},
   "source": [
    "**You can also customize sentence detection behavior** by using custom delimiters: https://spacy.io/usage/linguistic-features#sbd-custom\n",
    "For example, to deal with Zola's particular use of the exclamation mark.\n",
    "\n",
    "For the next example, you used the @Language.component(\"set_custom_boundaries\") decorator to define a new function that takes a Doc object as an argument. The job of this function is to identify tokens in Doc that are the beginning of sentences and mark their `.is_sent_start` attribute to True. Once done, the function must return the Doc object again.\n",
    "\n",
    "Then, you can add the custom boundary function to the Language object by using the `.add_pipe()` method.  \n",
    "Parsing text with this modified Language object will not treat the exclamation mark as an end-of-sentence marker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a734fad-3ec8-42ea-9c8d-23e2add25caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.language import Language\n",
    "@Language.component('set_custom_boundaries')\n",
    "def set_custom_boundaries(doc):\n",
    "    for token in doc[:-1]:\n",
    "        if token.text in ('!', '’', '«'):\n",
    "            doc[token.i+1].is_sent_start = False\n",
    "    return doc\n",
    "\n",
    "custom_nlp = fr_core_news_md.load()\n",
    "custom_nlp.add_pipe(\"set_custom_boundaries\", before=\"parser\")\n",
    "zola_doc = custom_nlp(pathlib.Path(file_path).read_text(encoding=\"utf-8\"))\n",
    "for sentence in zola_doc.sents:\n",
    "    print(f'{sentence[:10]}//')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db3d350-0537-4c02-a6e9-835a0d815856",
   "metadata": {},
   "source": [
    "## Stopwords\n",
    "\n",
    "Stop words are typically defined as the most common words in a language.\n",
    "\n",
    "With NLP, stop words are generally removed because they aren’t significant, and they heavily distort any word frequency analysis. For example, for **topic modeling**. But this is not always the case. Computational methods of **text attribution** (automatic author identification) rely precisely on the analysis of stop words only. In this case, you may wish to keep only these stop words and delete the others.\n",
    "\n",
    "SpaCy stores a list of stop words for the different languages.  \n",
    "See : https://machinelearningknowledge.ai/tutorial-for-stopwords-in-spacy/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8fec8c-9a18-44fc-8484-a1f1d27c191d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.fr.stop_words import STOP_WORDS as fr_stop\n",
    "print(fr_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88181e1-1ff4-41b0-8fba-422bad25a9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use sorted() to sort the set\n",
    "print(sorted(list(fr_stop))[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83da6b2c-bc81-4e2d-b4c6-17f7d07f2019",
   "metadata": {},
   "source": [
    "This list is not the best… You may need to modify it: add/remove stop words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914218da-e3e5-4035-991e-eec79fc16a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'plupart', 'aucuns' and 'tantôt'  are not part of the default list; they are added.\n",
    "from spacy.lang.fr.stop_words import STOP_WORDS as fr_stop_custom\n",
    "\n",
    "fr_stop_custom.add('plupart')\n",
    "fr_stop_custom |= {'aucuns','tantôt'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f0a0b7-e782-44c8-a950-2b5efc926c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete word(s) from list: : parler, specifique\n",
    "fr_stop_custom.remove('ouias')\n",
    "fr_stop_custom -= {'parler', 'specifique'}\n",
    "len(fr_stop_custom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59656a87-cc51-4ec9-bd99-fbe07b036a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can easily keep the only stop words by making use of the .is_stop attribute of each token:\n",
    "print([token for token in zola_doc[:500] if token.is_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93d5fe8-229c-4791-b11c-045767ab3650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversely, it's just as easy to keep only the full words that carry the semantics of the text.:\n",
    "print([token for token in zola_doc[:500] if not token.is_stop and not token.is_punct and not token.is_space])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4194b3-504c-42d4-8d53-ce79c877c9c2",
   "metadata": {},
   "source": [
    "## Word Frenquency\n",
    "\n",
    "From this, we can calculate words frequency lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272db258-ffad-4df9-853c-834914482489",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "words = [\n",
    "    token.text\n",
    "    for token in zola_doc\n",
    "    if not token.is_punct and not token.is_space\n",
    "]\n",
    "\n",
    "print(Counter(words).most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bd83ee-7adc-4d33-a8b1-547ce5904055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa541b19-ad43-408b-b2e3-4339f2b0d06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "words = [\n",
    "    token.text\n",
    "    for token in zola_doc\n",
    "    if not token.is_punct and not token.is_space\n",
    "]\n",
    "\n",
    "donnees = Counter(words).most_common(20)\n",
    "\n",
    "# Extract labels and values from tuples\n",
    "etiquettes = [x[0] for x in donnees]\n",
    "valeurs = [x[1] for x in donnees]\n",
    "\n",
    "# x\n",
    "indices = range(len(donnees))\n",
    "\n",
    "# Draw histogram\n",
    "plt.bar(indices, valeurs)\n",
    "\n",
    "# Add lables\n",
    "plt.xticks(indices, etiquettes)\n",
    "\n",
    "# Display\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0b7062-261d-4d19-9b69-f78b80d8bc65",
   "metadata": {},
   "source": [
    "It doesn't tell us much... 'de' is the most frequent word in the French language and frequency distribution conforms to the [Zipf's law](https://en.wikipedia.org/wiki/Zipf%27s_law).  \n",
    "That's why we need stopwords!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82777324-9e58-4ee4-b3fc-b2c429e7e548",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\n",
    "    token.text\n",
    "    for token in zola_doc\n",
    "    if not token.is_stop and not token.is_punct and not token.is_space\n",
    "]\n",
    "\n",
    "print(Counter(words).most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ef762f-a7aa-4f23-a48e-8d39440ce498",
   "metadata": {},
   "source": [
    "It's easy to see what we're talking about, and a historian would no doubt identify the text from this list alone.  But conjugation and plural further truncate the results. We need to lemmatize."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d21a39c-3f49-4614-8bc8-ca8245cd0a5f",
   "metadata": {},
   "source": [
    "## Lemmatization\n",
    "\n",
    "Lemmatization is the process of reducing inflected forms of a word. This reduced form (or root word = a dictionary entry), is called a lemma.\n",
    "\n",
    "For example, loves, loved and loving are all forms of 'love' lemma. The inflection of a word allows you to express different grammatical categories, like tense (loved vs love), number (lover vs lovers), and so on. Lemmatization is necessary because it helps you reduce the inflected forms of a word so that they can be analyzed as a single item. It can also help you normalize the text.\n",
    "\n",
    "spaCy puts a `lemma_ attribute` on the Token class. This attribute has the lemmatized form of the token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abee865-05b9-4f26-bf1d-ab3acef8267e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in zola_doc[36:150]:\n",
    "    if str(token) != str(token.lemma_):\n",
    "        print(f\"{str(token):>20} : {str(token.lemma_)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381a8470-9e36-4971-ad0a-6a98f9418748",
   "metadata": {},
   "source": [
    "**Let's do our sums.**\n",
    "\n",
    "[Counter](https://docs.python.org/3/library/collections.html#collections.Counter) is a subclass of dict that's specially designed for counting hashable objects in Python. It's a dictionary that stores objects as keys and counts as values.\n",
    "\n",
    "Just pass to the Counter the list of words to count, and then call the [.most_common()](https://docs.python.org/3/library/collections.html#collections.Counter.most_common) method and that's it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3821259c-77ed-48bd-a180-9bb1504a6d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\n",
    "    token.lemma_\n",
    "    for token in zola_doc\n",
    "    if not token.is_stop and not token.is_punct and not token.is_space\n",
    "]\n",
    "\n",
    "print(Counter(words).most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8133e96-6185-4fcf-8b1c-a344aab68343",
   "metadata": {},
   "source": [
    "Note, for example, that all conjugations of \"vouloir\" are reduced to its lemma. If you don't lemmatize the text, 'veux' and 'voulais' will be counted as different words, even though they both refer to the same concept. By lemmatizing, you can avoid duplicate words that may overlap conceptually.\n",
    "\n",
    "This model for French isn't incredible: the counts would make more sense if the lemmatization were better. But still, it's a useful first indication of the topics addressed by the text: we understand that it's about Dreyfus, justice and truth.\n",
    "\n",
    "With lemmatization, we can usually also recover morphosyntactic labels, which allow us to filter counts according to word (grammatical) category. For instance, what is the most frequent common noun?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8e7498-74af-4c53-97b7-59b77584e9ec",
   "metadata": {},
   "source": [
    "## Part-of-Speech Tagging\n",
    "\n",
    "Part-of-speech (POS) tagging is the process of assigning a POS tag to each token depending on its usage in the sentence. POS tags are useful for assigning a syntactic category like noun or verb to each word.\n",
    "\n",
    "In spaCy, POS tags are available as an attribute (`pos_`) on the Token object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32d4ff6-2f33-4802-b382-27e101d84978",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"{'text':15}\"\n",
    "    f\"{'lemma':15}\"\n",
    "    f\"{'pos':10}\"\n",
    "    f\"{'pos_explanation':10}\"\n",
    ")\n",
    "\n",
    "for token in zola_doc[:20]:\n",
    "    if not token.is_space:\n",
    "        print(\n",
    "            f\"{str(token.text):15}\"\n",
    "            f\"{str(token.lemma_):15}\"\n",
    "            f\"{str(token.pos_):10}\"\n",
    "            f\"{str(spacy.explain(token.pos_)):10}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5726f37b-d4c5-443e-8cb9-c31f69ee4f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns = [\n",
    "    token.lemma_\n",
    "    for token in zola_doc\n",
    "    if token.pos_ == \"NOUN\"\n",
    "]\n",
    "print(Counter(nouns).most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9dca6a3-0efd-4569-af5b-ac69b4b43263",
   "metadata": {},
   "source": [
    "Not bad. But let's try to format our output better, by storing the counts in a dataframe that we can easily manipulate later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b664d76-b50d-4281-b73b-aa2fa42d63dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "nouns_tally = Counter(nouns)\n",
    "nouns_df = pd.DataFrame(nouns_tally.most_common(), columns=['nouns', 'count'])\n",
    "nouns_df.iloc[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa544a64-96b9-4aac-ba9b-02636fa02721",
   "metadata": {},
   "source": [
    "Note that [displacy](https://spacy.io/usage/visualizers) makes it easy to build diagrams, which can look very serious in your thesis or article... 🙄\n",
    "\n",
    "For instance the dependency visualizer, [dep](https://spacy.io/usage/visualizers#dep), shows part-of-speech tags and syntactic dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be8d4c0-b450-4383-8726-a3814c2138db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a816be9-f08e-41e1-bf44-9d9e993e07de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# displaCy options : https://spacy.io/api/top-level#displacy_options\n",
    "\n",
    "s=20 # a counter to select a sentence (here the 20th)\n",
    "i=0\n",
    "for sentence in zola_doc.sents:\n",
    "    if i==s:\n",
    "        displacy.render(\n",
    "            sentence,\n",
    "            style=\"dep\",\n",
    "            jupyter=True,\n",
    "            options={'distance': 100, 'compact':False}\n",
    "        )\n",
    "    elif i>s:\n",
    "        break\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414f98e1-d44a-4278-a601-dc6a8526f587",
   "metadata": {},
   "source": [
    "## Named-Entity Recognition\n",
    "\n",
    "We've seen that counting common nouns is a good indicator of the topics. What if we could count the people or places mentioned?\n",
    "\n",
    "Named-entity recognition (NER) is the process of locating named entities and then classifying them into predefined categories, such as person names, locations, organizations.\n",
    "\n",
    "Let's see if the NER helps us to better understand the meaning of our text.  \n",
    "spaCy has the property `.ents` on Doc objects. You can use it to extract named entities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bef8e6-bbda-44cd-8494-76462de0bda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ent in zola_doc[2100:2200].ents:\n",
    "    print(\n",
    "        f\"\"\"\n",
    "        {ent.text = }\n",
    "        {ent.label_ = }\n",
    "        type = {spacy.explain(ent.label_)}\"\"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ed5471-8b1e-483a-9f4c-21d647ce967d",
   "metadata": {},
   "source": [
    "In the above example, `ent` is a [Span object](https://spacy.io/api/span) with various attributes:\n",
    "\n",
    "- `.text` gives the Unicode text representation (the string) of the entity.\n",
    "- `.label_` gives the label of the entity.\n",
    "- `.start_char` denotes the character offset for the start of the entity.\n",
    "- `.end_char` denotes the character offset for the end of the entity.\n",
    "\n",
    "`spacy.explain()` gives descriptive details about each entity label."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0568d583-9e36-44f8-abfd-8f0fbfddccb5",
   "metadata": {},
   "source": [
    "Counting people: for a better understanding, we propose 2 methods:\n",
    "\n",
    "- The first with a `for` loop, to understand how to read the list of entities.\n",
    "- La seconde (List comprehension) is more pythonic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11b7e53-0d04-4e3f-a55c-558b88a0b0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# method 1:\n",
    "people = []\n",
    "for named_entity in zola_doc.ents:\n",
    "    if named_entity.label_ == \"PER\":\n",
    "        people.append(named_entity.text)\n",
    "\n",
    "print(Counter(people).most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296c1bc3-b801-45e8-83ee-792a7c8f6c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# method 2 (pythonic):\n",
    "people = [\n",
    "    entity.text\n",
    "    for entity in zola_doc.ents\n",
    "    if entity.label_ == 'PER'\n",
    "]\n",
    "people_df = pd.DataFrame(Counter(people).most_common(20), columns=['character', 'count'])\n",
    "people_df.iloc[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56de18c-b6c8-4193-ad2e-182c12a7e474",
   "metadata": {},
   "source": [
    " You can also use displaCy to visualize these entities. Here, we're only visualizing a few sentences (`list(zola_doc.sents)[116:120]`), but it is of course possible to annotate the entire text (`zola_doc`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2efb4b-22e1-4bf1-bb10-972e9e042222",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "displacy.render(list(zola_doc.sents)[116:120], style='ent')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7750785b-1f26-4b50-a0dc-5844e6df607e",
   "metadata": {},
   "source": [
    "## Named-Entity Linking\n",
    "\n",
    "Named-entity recognition is very useful. It enables us to get entities. But we can't identify them. We know that a person is named 'Picquart', but how do we know who he is? How do we resolve the inevitable ambiguities? -Namesakes are common…\n",
    "\n",
    "This is the purpose of linking: to try to assign a shared identifier to the entity (e.g. Wikidata). So we learn that 'Picquart' is 'Marie-Georges Picquart (1854-1914)', a key player in the Dreyfus affair. Thanks to this linkage, we can also automatically retrieve information about the person, via APIs!\n",
    "\n",
    "Entity Linking is a difficult task, and there are many different strategies. We present here [spaCy fishing](https://github.com/Lucaterre/spacyfishing), a spaCy wrapper for entity-fishing, a tool for named entity recognition, linking and disambiguation against Wikidata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b689e3-99a0-4063-af90-1e22ae9b3a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install spacyfishing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40363c81-5e9a-421c-96bd-22e9448ca7d2",
   "metadata": {},
   "source": [
    "### French"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d1ec96-edd1-4cfd-b7c4-f04d0676a829",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_fr = spacy.load(\"fr_core_news_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88586891-6b7f-402d-b2ed-3ccf54f83ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default (but the service is often down...)\n",
    "nlp_fr.add_pipe(\"entityfishing\", config={'language':'fr'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b51c78-1391-41c6-822d-e02fe469b5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# same, using huma-num instance:\n",
    "'''\n",
    "nlp_fr.add_pipe(\"entityfishing\", config={\n",
    "    'language':'fr',\n",
    "    'api_ef_base': 'http://nerd.huma-num.fr/nerd/service'\n",
    "})\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040b9839-f09e-4c24-b99a-c86ad9438783",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nlp_fr.pipe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5685b12-7d3d-42d9-ac95-9890915f81c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check service\n",
    "zola_doc_fr._.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74416601-aa28-428e-95e7-8edf909bf558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import text\n",
    "import pathlib\n",
    "file_path = './data/zola_accuse_fr.txt'\n",
    "zola_doc_fr = nlp_fr(pathlib.Path(file_path).read_text(encoding=\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb67d9de-6033-46c6-b639-e07f5c4141ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ent in zola_doc_fr.ents:\n",
    "    print((ent.text, ent.label_, ent._.kb_qid, ent._.url_wikidata, ent._.nerd_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb339f3-fcc3-4afb-9f2f-cf6499187798",
   "metadata": {},
   "source": [
    "spaCy fishing allows you to link several knowledge bases, [collecting information from Wikidata](https://github.com/Lucaterre/spacyfishing#get-extra-information-from-wikidata):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68727c8-4e17-46bf-b392-ac8f849cb351",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_fr = spacy.load(\"fr_core_news_md\")\n",
    "nlp_fr.add_pipe(\"entityfishing\", config={\n",
    "    'language':'fr',\n",
    "    'extra_info': True\n",
    "})\n",
    "for ent in zola_doc_fr.ents:\n",
    "    if ent.label_ == 'PER':\n",
    "        print((ent.text, ent.label_, ent._.kb_qid, ent._.url_wikidata, ent._.nerd_score, ent._.other_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13701a98-bde1-4692-b099-154551bf1f5f",
   "metadata": {},
   "source": [
    "At last but not at least, you can display the result in a very convenient way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29c1564-dde5-4745-8797-bd63b91ae0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = {\n",
    "    \"ents\": [\"MISC\", \"LOC\", \"PER\"],\n",
    "    \"colors\": {\"LOC\": \"#82e0aa\", \"PER\": \"#85c1e9\", \"MISC\": \"#f0b27a\"}\n",
    "}\n",
    "\n",
    "params = {\n",
    "    \"text\": zola_doc_fr.text,\n",
    "    \"ents\": [\n",
    "        {\n",
    "            \"start\": ent.start_char,\n",
    "            \"end\": ent.end_char,\n",
    "            \"label\": ent.label_,\n",
    "            \"kb_id\": ent._.kb_qid,\n",
    "            \"kb_url\": ent._.url_wikidata\n",
    "        }\n",
    "        for ent in zola_doc_fr.ents\n",
    "    ],\n",
    "    \"title\": None\n",
    "}\n",
    "\n",
    "spacy.displacy.render(params, style=\"ent\", manual=True, options=options, jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9dcbdd-9a79-48c3-b87d-e3d06cf21715",
   "metadata": {},
   "source": [
    "### English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b23a0bf-3524-4c83-a116-c0b5e8bbdd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda install -c conda-forge spacy-model-en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6865f3a1-5620-4772-b532-eeec5fc9efd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_en = spacy.load(\"en_core_web_md\")\n",
    "nlp_en.add_pipe(\"entityfishing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6ec14d-b530-43e6-8ece-ede32fc480c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "file_path = './data/zola_accuse_en.txt'\n",
    "zola_doc_en = nlp_en(pathlib.Path(file_path).read_text(encoding=\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6a60ae-30c2-4f42-b2d7-113b4cfd3904",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ent in zola_doc_en.ents:\n",
    "    if ent.label_ == 'PERSON':\n",
    "        print((ent.text, ent.label_, ent._.kb_qid, ent._.url_wikidata, ent._.nerd_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2cd4d3-4a63-4b1e-a386-d712a9f03ab6",
   "metadata": {},
   "source": [
    "## Preprocessing Functions\n",
    "\n",
    "To bring your text into a format ideal for analysis, you can write preprocessing functions to encapsulate your cleaning process. For example, in this section, you’ll create a preprocessor that applies the following operations:\n",
    "\n",
    "- Lowercases the text\n",
    "- Lemmatizes each token\n",
    "- Removes punctuation symbols\n",
    "- Removes stop words\n",
    "\n",
    "A preprocessing function converts text to an analyzable format. It’s typical for most NLP tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541933a6-5d7a-4229-88c7-be3e43534c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "def preprocess_lemma(token):\n",
    "    return token.lemma_.strip().lower()\n",
    "\n",
    "# Filter: a function that returns True or False for a token according to certain criteria\n",
    "def is_token_allowed(token):\n",
    "    return bool(\n",
    "        token\n",
    "        and str(token).strip()\n",
    "        and not token.is_stop\n",
    "        and not token.is_punct\n",
    "    )\n",
    "\n",
    "filtered_zola_lemmas = [\n",
    "    preprocess_lemma(token)\n",
    "    for token in zola_doc\n",
    "    if is_token_allowed(token)\n",
    "]\n",
    "\n",
    "print(filtered_zola_lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddac781-f143-4f9e-8d67-8f8cc1bb2e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"{'text':15}\"\n",
    "    f\"{'is_allowed':15}\"\n",
    ")\n",
    "\n",
    "for token in zola_doc[:10]:\n",
    "    print(\n",
    "        f\"{str(token.text):15}\"\n",
    "        f\"{str(is_token_allowed(token)):15}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac225f5-d2b2-4319-a0c3-36510e8c83e4",
   "metadata": {},
   "source": [
    "## Appendix. Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26218fe-58a8-4c64-95b0-7258b2665849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all imports\n",
    "'''\n",
    "import spacy\n",
    "from spacy.lang.fr.stop_words import STOP_WORDS as fr_stop\n",
    "from spacy import displacy\n",
    "import fr_core_news_md\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "from collections import Counter\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f470c371-c3df-4929-a226-27808e24e4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html#sharing-an-environment\n",
    "#!conda env export"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
