{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Textual data scraping and preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This course is a reworking of the excellent book designed by Melanie Walsh, [*Introduction to Cultural Analytics & Python*](https://melaniewalsh.github.io/Intro-Cultural-Analytics/welcome.html). Many paragraphs and explanations have been retained without modification.\n",
    "\n",
    "> **Read this book!**, itself inspired by *Inspired by web scraping lessons from [Lauren Klein](https://github.com/laurenfklein/emory-qtm340/blob/master/notebooks/class4-web-scraping-complete.ipynb) and [Allison Parrish](https://github.com/aparrish/dmep-python-intro/blob/master/scraping-html.ipynb)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classically, a distinction is made between different work stages: (1) data production, (2) data processing and (3) data analysis.\n",
    "This course is a practical and fairly detailed introduction to the first 2 stages: **production and (pre)processing**.\n",
    "\n",
    "You'll learn how to use open Web services to automatically collect textual data. In doing so, you'll gain a better understanding of how the Web works (HTTP, HTML), discover structured data formats (CSV, JSON, XML) and get hands-on experience of the Python programming language.\n",
    "\n",
    "The important thing is not necessarily to retain everything, but to gain a better understanding of how this ecosystem works, so that you can gradually determine by yourself the solutions you need to implement to meet your own requirements.\n",
    "\n",
    "\n",
    "In this lesson, we're going to introduce :\n",
    "\n",
    "1. how to \"scrape\" data from the internet with the Python libraries requests and BeautifulSoup.\n",
    "1. how to preprocess our datas with spaCy.\n",
    "\n",
    "We will cover how to:\n",
    "\n",
    "* Programmatically access the text of a web page\n",
    "* Extract informations from structured documents (CSV/TSV, HTML, JSON, XML)\n",
    "* Build collections of texts\n",
    "* Design pre-processings\n",
    "\n",
    "And along the way, we'll be learning the basics of the Python programming language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Do We Need To Scrape At All?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today, written heritage is massively available on the Internet under Free Licenses.\n",
    "\n",
    "Community initiatives such as Project Gutenberg, based on crowdsourcing, have been succeeded by very large-scale institutional projects exploiting the potential of machine learning for the automatic acquisition of text, including handwritten. Gallica (BnF's digital library) represents over 10 million documents available online.\n",
    "\n",
    "Here are a few projects worth your attention :\n",
    "\n",
    "- [Project Gutenberg](https://www.gutenberg.org/): since December 1971! Project Gutenberg is a library of free electronic versions of physically existing books (>70,000 free eBooks).\n",
    "- [Wikisource](https://fr.wikisource.org/wiki/Wikisource:Accueil): Wikisource is a digital library of public domain texts, managed as a wiki using the MediaWiki engine (> 360,000 free and open texts).\n",
    "- [Gallica](https://gallica.bnf.fr/): Gallica is the digital library of the Bibliothèque nationale de France and its partners. It has been freely accessible since 1997, and contains > 10 million documents.\n",
    "- [HathiTrust](https://www.hathitrust.org/)\n",
    "- …\n",
    "\n",
    "This digital heritage opens up unprecedented prospects for the humanities: provided, of course, that we know how to use these services in order to build research corpora. Text is also increasingly natively digital, and researchers often need to automatically follow certain subjects on platforms such as Twitter.\n",
    "\n",
    "This course will not teach you how to analyze these huge textual corpora, but rather how to build them. It will also show you how to pre-process them, an essential prerequisite for computational analysis.\n",
    "\n",
    "**Building the collection of Jules Verne novels**\n",
    "\n",
    "Let's start by building up a small corpus of Jules Verne's works available on Project Gutenberg. In the process, we'll discover HTTP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading a metadata table using Pandas (CSV/TSV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![csv](./img/verne_csv.png)\n",
    "\n",
    "A [comma-separated values](https://en.wikipedia.org/wiki/Comma-separated_values) (CSV) file is a delimited text file that uses a comma to separate values. Each line of the file is a data record. Each record consists of one or more fields, separated by commas. The use of the comma as a field separator is the source of the name for this file format. A CSV file typically stores tabular data (numbers and text) in plain text, in which case each line will have the same number of fields.\n",
    "\n",
    "The T of TSV is for 'tabulation'. Tabs are more convenient for separating text values, which may themselves contain commas...\n",
    "In short, our lesson begins with the reading of a simple TSV metadata table, which is a very common data exchange format.\n",
    "\n",
    "=====\n",
    "\n",
    "**[Pandas](https://pandas.pydata.org/)** is a library written for the Python programming language, enabling data manipulation and analysis. In particular, it offers data structures and array manipulation operations.\n",
    "\n",
    "Here, we use Pandas to read a [TSV table](https://en.wikipedia.org/wiki/Comma-separated_values) and store its data in a [DataFrame](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html) (a 2-dimensional array), so that it can be manipulated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = pd.read_csv(\"data/verne.csv\", delimiter='\\t', encoding='utf-8')\n",
    "# Get an overview\n",
    "urls.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the list of books and their metadata (download link, language) has been compiled in advance. We'll see later how to collect this information automatically, using a search API such as that provided by BnF.\n",
    "\n",
    "Let's learn how to read the DataFrame in different ways and to access the values contained in the cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Knowing the size of the df\n",
    "print(\n",
    "    f\"{'Dimensions':15}\"\n",
    "    f\"{'Lines':15}\"\n",
    "    f\"{'Columns'}\"\n",
    ")\n",
    "print(\n",
    "    f\"{str(urls.shape):15}\"\n",
    "    f\"{str(urls.shape[0]):15}\"\n",
    "    f\"{str(urls.shape[1])}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access one or more specific lines or slices using the [`.iloc[]`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.iloc.html) property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access a specific line\n",
    "urls.iloc[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access specific lines\n",
    "urls.iloc[[5, 7, 11]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access a slice of lines\n",
    "urls.iloc[5:8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get cell value by index or name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    str(urls.iloc[5][2]),\n",
    "    '<=>',\n",
    "    str(urls.iloc[5]['Title'])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each novel title in this TSV file is paired with a URL for the plain text. How can we actually use these URLs to get computationally tractable text data? Though we could manually navigate to each URL and copy/paste each screenplay into a file, that would be suuuuper slow and painstaking. It would be much better to programmatically access the text data attached to every URL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HTTP Requests and Responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To programmatically access the text data attached to every URL, we can use a Python library called [requests](https://requests.readthedocs.io/en/master/).\n",
    "\n",
    "A [Uniform Resource Locator](https://en.wikipedia.org/wiki/URL) (URL, ~ a web address), is a reference to a web resource that specifies its location on a computer network and a mechanism for retrieving it.\n",
    "\n",
    "Every HTTP URL conforms to the syntax of a generic URI. The URI generic syntax consists of 6 components organized hierarchically:\n",
    "\n",
    "`http://www.domain.com:80/path/to/myfile.html?key1=value1&key2=value2#anchor_in_doc`\n",
    "\n",
    "URLS parts:\n",
    "\n",
    "- **protocol**: `http://` or `https://`.\n",
    "- **domain name**: `www.domain.com` –instead of a domain name, you can use an IP address.\n",
    "- **port**: `:80` –indicates the technical \"door\" to be used to access the server's resources. This fragment is generally absent, as the browser uses the standard ports associated with the protocols (80 for HTTP, 443 for HTTPS).\n",
    "- **path**: `/path/to/myfile.html` –path, on the web server, to the resource. In the early days of the Web, this path often corresponded to a \"physical\" path existing on the server. Today, this path is merely an abstraction managed by the web server, and no longer corresponds to a \"physical\" reality.\n",
    "- **parameters**: `?key1=value1&key2=value2` –constructed as a list of key/value pairs separated by an ampersand.\n",
    "- **anchor**: `#anchor_in_doc` –points to a given location in the resource.\n",
    "\n",
    "When you type in a URL in your search address bar, you're sending an HTTP **request** for a web page, and the server which stores that web page will accordingly send back a **response**, some web page data that your browser will render.\n",
    "\n",
    "<img src=\"./img/http.png\" width=\"600px\">\n",
    "\n",
    "In the image below, in the inspector's network tab, you can see that for the URL, 2 HTTP requests received a positive response (status 200)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![404](./img/request-response.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get HTML Data with Requests\n",
    "\n",
    "**HTTP**\n",
    "\n",
    "In the HTTP protocol, a **method** is a command specifying a type of request, i.e. asking the server to perform an action. In general, the action concerns a resource identified by the URL following the method name.\n",
    "\n",
    "There are many [methods](https://en.wikipedia.org/wiki/HTTP#HTTP/1.1_request_messages), the most common being `GET`, `HEAD` and `POST` :\n",
    "\n",
    "- `GET`: the most common method for requesting a resource. A GET request has no effect on the resource.\n",
    "- `HEAD`: this method only requests information about the resource, without actually requesting the resource itself.\n",
    "- `POST`: this method is used to send data for processing (usually from an HTML form).\n",
    "\n",
    "**Requests Python Library**\n",
    "\n",
    "With the [`.get()` method](https://requests.readthedocs.io/en/latest/api/#requests.get), we can request to \"get\" web page data for a specific URL, which we will store in a varaible called `response`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(\"https://www.gutenberg.org/cache/epub/4791/pg4791.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HTTP Header Fields\n",
    "\n",
    "Wikipedia: \"[HTTP header fields](https://en.wikipedia.org/wiki/List_of_HTTP_header_fields) are a **list of strings sent and received by both the client program and server on every HTTP request and response. These headers are usually invisible to the end-user and are only processed or logged by the server and client applications. They define how information sent/received through the connection are encoded** (as in Content-Encoding), the session verification and identification of the client (as in browser cookies, IP address, user-agent) or their anonymity thereof (VPN or proxy masking, user-agent spoofing), how the server should handle data (as in Do-Not-Track), the age (the time it has resided in a shared cache) of the document being downloaded, amongst others.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, using the `headers` [`Response` object](https://requests.readthedocs.io/en/latest/user/advanced/#request-and-response-objects); we can see that the document returned by Project Gutenberg is a plain text file.\n",
    "\n",
    "But more often than not, as we'll soon see, responses are encoded in HTML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.headers['Content-Type']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HTTP Status Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we check out `response`, it will simply tell us its [HTTP response code](https://developer.mozilla.org/en-US/docs/Web/HTTP/Status), aka whether the request was successful or not.\n",
    "\n",
    "\"200\" is a successful response, while \"404\" is a common \"Page Not Found\" error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what happens if we make a mistake entering the URL...  \n",
    "('page4791' instead of 'pg4791')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_response = requests.get(\"https://www.gutenberg.org/cache/epub/4791/page4791.txt\")\n",
    "bad_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![404](./img/bad_response.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Text From Web Page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To actually get at the text data in the reponse, we need to use [`.text` property](https://requests.readthedocs.io/en/latest/api/#requests.Response.text), which we will save in a variable called `text_string`.\n",
    "\n",
    "Project Gutenberg provides here a version of the novel in plain text format (what is very convenient from a pedagogical point of view). But more often, the text data that we're getting on the Web is formatted in the HTML markup language, which we will talk more about in the BeautifulSoup section below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_string = response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the text of the novel now in a variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [],
   "source": [
    "print(text_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Text From Multiple Web Pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeating the operation for each novel would be tedious… Let's see how we can extract the text for every URL in the DataFrame at once. To do so, we're going to create a smaller DataFrame containing the first 10 novels –fewer processings for the demonstration and the planet…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_urls = urls[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to make a function called `scrape_novel()` that includes our `requests.get()` and `response.text` code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_novel(url):\n",
    "    response = requests.get(url)\n",
    "    html_string = response.text\n",
    "    return html_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we're going to apply it to the \"url\" column of the DataFrame and create a new \"text\" column for the resulting extracted text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_urls['text'] = sample_urls['url'].apply(scrape_novel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_urls.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DataFrame above is truncated, so we can't see the full contents of the \"text\" column. But if we print out every row in the column, we can see that we successfully extracted text for each URL (though some of these URLs returned 404 errors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [],
   "source": [
    "print(sample_urls.iloc[4]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's simple and easy! However, that plain text format poses a few problems. It is not possible to automatically distinguish the Jules Verne text from the metadata and editorial paratext. Likewise, all the credit references at the end of the transcription are mixed in with the text, which can skew the analysis.\n",
    "\n",
    "We need a more structured format that allows us to distinguish between the author's text, the metadata and the editorial paratext."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not all web pages will be as easy to scrape as these Gutenberg project plain text files, however. Let's say we wanted to scrape the lyrics for NTM's song \"[On est encore là](https://genius.com/Supreme-ntm-on-est-encore-la-lyrics)\" (1998) from Genius.com.\n",
    "\n",
    "NB. The teaching sequence is by Mélanie Walsh, and has been updated as the data model has evolved..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/ntm.png\" class=\"center\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even at a glance, we can tell that this *Genius* web page is a lot more complicated than the *Gutengerg project* page and that it contains a lot of information beyond the lyrics.\n",
    "\n",
    "Sure enough, if we use our requests library again and try to grab the data for this web page, the underlying data is much more complicated, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": [
     "output_scroll",
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "response = requests.get(\"https://genius.com/Supreme-ntm-on-est-encore-la-lyrics\")\n",
    "html_string = response.text\n",
    "print(html_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we extract just the song lyrics from this messy soup of a document? Luckily there's a Python library that can help us called [BeautifulSoup](https://beautiful-soup-4.readthedocs.io/en/latest/), which parses HTML documents.\n",
    "\n",
    "To understand BeautifulSoup and HTML, we're going to take things one step at a time. We'll start with a very simple example (beginner level) to understand the basic structure of an HTML page. Next, we'll go back to the Jules Verne novels available on Project Gutenberg (intermediate level) before trying to automatically extract the lyrics to the NTM song (advanced level)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HTML5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This [Singers' singers webpage](todolink) is adapted from Parrish's website titled \"[Kittens and the TV Shows They Love](http://static.decontextualize.com/kittens.html)\" made for the purposes of teaching BeautifulSoup.\n",
    "\n",
    "Instant geek: thanks to IPython's [core.display module](https://ipython.org/ipython-doc/2/api/generated/IPython.core.display.html), it's even possible to display the content of a web page in a notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "response = requests.get(\"https://raw.githubusercontent.com/architexte/cours-data-processing/main/data/punk.html\")\n",
    "html_string = response.text\n",
    "display(HTML(html_string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the structure of this HTML page:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(html_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HTML Tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HTML stands for HyperText Markup Language. It is the standard language for writing web page documents. The most important thing you need to know about HTML is that the language uses HTML \"tags\" to represent different elements, such as a main header `<h1>`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| HTML Tag                | Explanation                              |\n",
    "|--------------------|-------------------------------------------|\n",
    "| <\\!DOCTYPE>        | Defines document type                 |\n",
    "| <html\\>             | Root of the HTML document                  |\n",
    "| <head\\>             | Metadata about document    |\n",
    "| <title\\>            | Title for document          |\n",
    "| <body\\>             | Document body               |\n",
    "| <h1\\> to <h6\\>       |  Headings                    |\n",
    "| <div\\> | Bloc section in a document                   |\n",
    "| <p\\>                | Paragraph                       |\n",
    "| <ul\\> | Unordered list                     |\n",
    "| <ol\\> | Ordered list                     |\n",
    "| <li\\> | List item                     |\n",
    "| <br\\>               | Line break               |\n",
    "| <a\\> | Hyperlink                       |\n",
    "| <img\\> | Image                         |\n",
    "| <span\\> | Inline section in a document                   |\n",
    "| <\\!\\-\\-comment here-\\-> | Comment                         |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HTML tags often, but not always, require a \"closing\" tag. For example, the main header \"Kittens and the TV Shows They Love\" will be surrounded by `<h1>` (opening tag) and `</h1>` (closing tag) on either side: `<h1>Singers' singers</h1>`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HTML Attributes, Classes, and IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HTML elements sometimes come with even more information inside a tag. This will often be a keyword (like `class` or `id`) followed by an equals sign `=` and a further descriptor such as `<div class=\"iggy\">`.\n",
    "\n",
    "We need to know about tags as well as attributes, classes, and IDs because this is how we're going to extract specific HTML data with BeautifulSoup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BeautifulSoup (Singers’ singers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make a BeautifulSoup document, we call `BeautifulSoup()` with two parameters: the `html_string` from our HTTP request and [the kind of parser](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#specifying-the-parser-to-use) that we want to use, which will always be `\"html.parser\"` for our purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(\"https://raw.githubusercontent.com/architexte/cours-data-processing/main/data/punk.html\")\n",
    "html_string = response.text\n",
    "document = BeautifulSoup(html_string, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(document))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract HTML Element"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the [`.find()` method](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#find) to find and extract certain elements, such as a main header."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document.find(\"h1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want only the text contained between those tags, we can use `.text` to extract just the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document.find(\"h1\").text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(document.find(\"h1\").text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the HTML element that contains an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "document.find(\"img\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document.find(\"img\")['src']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The `.find()` method returns only one result** (the first one).  \n",
    "However, we may wish to obtain a list of all the images called up in the page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract Multiple HTML Elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also extract multiple HTML elements at a time with the [`.find_all()` method](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#find-all) that returns a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document.find_all(\"img\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Python, it's easy to use a `for` loop to go through the list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img in document.find_all(\"img\"):\n",
    "    print(img['src'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's possible to extract a serie of elements according to the value of their attributes (here, `div` whose class attribute value is `singer`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "document.find_all(\"div\", attrs={\"class\": \"singer\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document.find(\"h2\").text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document.find_all(\"h2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` {warning}\n",
    "Heads up! The code below will cause an error.\n",
    "```\n",
    "Let's try to extract the text from all the header2 elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "document.find_all(\"h2\").text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That didn't work! In order to extract text data from multiple HTML elements, we need a `for` loop and some list-building."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_h2_headers = document.find_all(\"h2\")\n",
    "all_h2_headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will make an empty list called `h2_headers`.\n",
    "\n",
    "Then `for` each `header` in `all_h2_headers`, we will grab the `.text`, put it into a variable called `header_contents`, then `.append()` it to our `h2_headers` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2_headers = []\n",
    "for header in all_h2_headers:\n",
    "    header_contents = header.text\n",
    "    h2_headers.append(header_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2_headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can produce the same result in a more \"pythonic\" way by using a **list comprehension** (shorter syntax):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "h2_headers = [header.text for header in all_h2_headers]\n",
    "h2_headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect HTML Elements with Browser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most times if you're looking to extract something from an HTML document, it's best to use your \"Inspect\" capabilities in your web browser. You can hover over elements that you're interested in and find that specific element in the HTML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/inspect1.png\" width=\"700px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, if we hover over the main link \"Johnny Cash\":\n",
    "\n",
    "<img src=\"./img/inspect2.png\" width=\"700px\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Turn! (Project Gutenberg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok so now we've learned a little bit about how to use BeautifulSoup to parse HTML documents. So how would we apply what we've learned to extract the text of Jules Verne's novels?\n",
    "\n",
    "Project Gutenberg shares its editions in plain text format, but not only. An HTML version is of course also available. It's a little more difficult to process it than the full-text version, but at least we can try to extract metadata and the author's text alone.\n",
    "\n",
    "Let's follow our example of the *Voyage au Centre de la Terre*: https://www.gutenberg.org/cache/epub/4791/pg4791.html.\n",
    "\n",
    "Inspect the page with your browser and try to extract:\n",
    "\n",
    "- the main header of the page\n",
    "- the title of the novel\n",
    "- `p` elements with `id``\n",
    "\n",
    "Finally, try to extract the only text of the novel (the one written by Jules Verne)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(\"https://www.gutenberg.org/cache/epub/4791/pg4791.html\")\n",
    "html_string = response.text\n",
    "document = BeautifulSoup(html_string, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main header\n",
    "document.find('h2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main header text\n",
    "document.find('h2').text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No `h1`elmement, it’s a semantic oddity... A closer look reveals that the hierarchy of headings is not even respected (`h5` > `h2`).\n",
    "\n",
    "But note that thanks to BeautifulSoup, you can easily print the text contained in all descendant elements (here `span`). How convenient!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Title of the novel\n",
    "document.find(\"h5\").text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First p element with an id attribute\n",
    "document.find(\"p\", {\"id\" : True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The `p` element whose id value is 'id02329'.\n",
    "document.find(\"p\", {\"id\" : \"id02329\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A `p` element that has an `id` attribute AND a `style` attribute.\n",
    "document.find(\"p\", attrs={\"id\" : True, \"style\" : True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same, compact syntax\n",
    "document.find(\"p\", {\"id\" : True, \"style\" : True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of the 10 first p elements with an id attribute\n",
    "document.find_all(\"p\", {\"id\" : True})[0:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#document.find_all(['p', 'h2', 'h5'])\n",
    "#document.find_all(['p', 'h2', 'h5'], id=True) # relou\n",
    "#document.select('p[id], h2, h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Better, but we don't want to extract either the acknowledgements (`@id` 'id00000'-'id00002') or the editorial notes (`@id` 'id00003'-'id00005')...  \n",
    "One strategy is to extract only the paragraphs following the novel's title (`h5`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_tag = document.find('h5')\n",
    "start_tag.find_all_next('p')[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're making progress, but on closer inspection, we realize that we're forgetting the titles (`h2`). You can pass a list of elements to the `find_all_next()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_tag = document.find('h5')\n",
    "start_tag.find_all_next(['p', 'h2'])[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All that's left is to save the text in a list, simply with a small loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verne_text_list = []\n",
    "for element in start_tag.find_all_next(['p', 'h2']):\n",
    "    text = element.text\n",
    "    verne_text_list.append(text)\n",
    "verne_text_list[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Otherwise, print it all in a single string.\n",
    "verne_p = document.find_all('p', id=True)\n",
    "for element in start_tag.find_all_next(['p', 'h2'])[0:3]:\n",
    "    print(element.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Issue**\n",
    "\n",
    "- find all `p` with id : `document.find_all('p', id=True)`\n",
    "\n",
    "This syntax prohibits the selection of paragraphs with `id` and `h2` without: impossible to write `('[p', id=True, h2])`…  \n",
    "In this case, you can use the `select()` method :\n",
    "\n",
    "- select all `p` with id and all `h2` : ` document.select('p[id], h2')`\n",
    "\n",
    "All you have to do is write:\n",
    "\n",
    "```python\n",
    "verne_text_list = [element.text for element in document.select('p[id], h2, h5')]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary**\n",
    "\n",
    "Thanks to HTML, BeautifulSoup and a little trickery, you can extract Jules Verne's text alone. A little more difficult than with the plain text format, but also more subtle: you manage to separate the author's text from the editorial paratext.\n",
    "\n",
    "Issue: this recipe works for this novel, but what guarantee do we have that it will work for other texts? To automate extractions, we need standardized sources... That's one of the advantages of APIs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Turn! (Genius)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So how would we apply what we've learned to extract NTM lyrics?\n",
    "\n",
    "https://genius.com/Supreme-ntm-on-est-encore-la-lyrics\n",
    "\n",
    "What HTML element do we need to \"find\" to extract the song lyrics?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(\"https://genius.com/Supreme-ntm-on-est-encore-la-lyrics\")\n",
    "html_string = response.text\n",
    "document = BeautifulSoup(html_string, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do we have in the `p` elements?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntm_p = document.find_all(\"p\")\n",
    "print(ntm_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What HTML element do we need to \"find\" to extract the title?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(document.find('h1').text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En inspectant la page, on s’aperçoit que certains éléments `div` ont un attribut `data-lyrics-container`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(document.find('div', {\"data-lyrics-container\": \"true\"}).text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Issue**. We lose the lines of verse. What can we do? The [`get_text()` method](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#get-text) returns all the text in a document (or beneath a tag) as a single Unicode string and enables to specify a string to be used to join the bits of text together…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics = document.find('div', {\"data-lyrics-container\": \"true\"}).get_text(\"\\n\")\n",
    "print(lyrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Issue (continued)**. Good idea, but it's really not great because of the segmentation of the annotations...  \n",
    "We need to be more specific and process all `br` elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for br in document.find_all(\"br\"):\n",
    "    br.replace_with(\"\\n\")\n",
    "lyrics = document.find('div', {\"data-lyrics-container\": \"true\"}).text\n",
    "print(lyrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Issue (end)**. Gee, we only have the beginning of the lyrics, which are written in several `div`..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics = document.find_all('div', {\"data-lyrics-container\": \"true\"})\n",
    "for lyrics_part in lyrics:\n",
    "    print(lyrics_part.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reuse**.  \n",
    "All that remains is to write this code into a small function so that we can reuse it to automatically extract the text of other songs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lyrics(song_genius_url):\n",
    "    response = requests.get(song_genius_url)\n",
    "    document = BeautifulSoup(response.text, \"html.parser\")\n",
    "    for br in document.find_all(\"br\"):\n",
    "        br.replace_with(\"\\n\")\n",
    "    lyrics = document.find_all('div', {\"data-lyrics-container\": \"true\"})\n",
    "    for lyrics_part in lyrics:\n",
    "        print(lyrics_part.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://genius.com/Grandmaster-flash-and-the-furious-five-the-message-lyrics\n",
    "# https://genius.com/De-la-soul-the-magic-number-lyrics\n",
    "# https://genius.com/Dr-jeckyll-and-mr-hyde-genius-rap-lyrics\n",
    "# https://genius.com/Supreme-ntm-on-est-encore-la-lyrics\n",
    "get_lyrics('https://genius.com/Amel-bent-ma-philosophie-lyrics')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**.\n",
    "\n",
    "Unfortunately, this method is neither reusable nor future-proof… **We need standardized data served via an API.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## APIs\n",
    "\n",
    "[Wikipedia](https://en.wikipedia.org/wiki/API): An **application programming interface** (API) is a way for two or more computer programs to communicate with each other. It is a type of software interface, offering a service to other pieces of software.  \n",
    "In contrast to a user interface, which connects a computer to a person, an application programming interface connects computers or pieces of software to each other. It is not intended to be used directly by a person (the end user) other than a computer programmer who is incorporating it into the software.\n",
    "\n",
    "- **An API enables a computer to request information from another computer over the Internet**.\n",
    "- **Data access endpoints and the format of the response are standardized according to a specification**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Genius API (JSON)\n",
    "\n",
    "According to its [documentation](https://docs.genius.com), the Genius API provides access to various resources, including:\n",
    "\n",
    "- Search (results)\n",
    "- Artists\n",
    "- Songs\n",
    "\n",
    "<img src=\"./img/genius_doc.png\" class=\"center\" >\n",
    "\n",
    "Let's explore the possibilities out of curiosity…\n",
    "\n",
    "Genius uses the OAuth2 standard for making API calls on behalf of individual users.  \n",
    "Requests are authenticated with an **Access Token** sent in an HTTP header or simply **as a request parameter**.\n",
    "\n",
    "[How to get, store and call your Genius API keys](https://melaniewalsh.github.io/Intro-Cultural-Analytics/04-Data-Collection/07-Genius-API.html#api-keys)…\n",
    "\n",
    "The best practice is to keep your API keys away from your code, such as in another file.\n",
    "\n",
    "My key is stored in a python file called `api_key.py` that contains just one variable `your_client_access_token = \"MY_API_KEY\"`, so I can import below this variable into this notebook with `import api_key`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import api_key\n",
    "#api_key.your_client_access_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making an API Request\n",
    "\n",
    "Making an API request looks a lot like typing a specially-formatted URL. But instead of getting a rendered HTML web page in return, you get some data in return.\n",
    "\n",
    "Let's start with the basic search, which allows you to get a bunch of Genius data about any artist or songs that you search for:\n",
    "\n",
    "http://api.genius.com/search?q={search_term}&access_token={client_access_token}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_term = \"Supreme NTM\"\n",
    "genius_search_url = f\"http://api.genius.com/search?q={search_term}&access_token={api_key.your_client_access_token}\"\n",
    "response = requests.get(genius_search_url)\n",
    "response.headers['Content-Type']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, the response is not formatted as plain text or HTML, but as JSON.\n",
    "With Requests, you can call [.json()](https://requests.readthedocs.io/en/latest/api/#requests.Response.json) to returns the json-encoded content of a response.\n",
    "\n",
    "Thanks to IPython's [`.display` module](https://ipython.readthedocs.io/en/stable/api/generated/IPython.display.html#IPython.display.JSON), we can effectively display a response that is quite long:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JSON(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### JSON\n",
    "\n",
    "[JSON](https://en.wikipedia.org/wiki/JSON) (JavaScript Object Notation) is an open standard file format and data interchange format that uses human-readable text to store and transmit data objects consisting of attribute–value pairs and arrays (or other serializable values). JSON is commonly used by APIs.  \n",
    "JSON data can be nested and contains key/value pairs.\n",
    "\n",
    "JSON [Syntax Rules](https://www.w3schools.com/whatis/whatis_json.asp):\n",
    "\n",
    "- Data is in name/value pairs: `'title': 'On est encore là'`\n",
    "- Data is separated by commas: `'id': 87367, 'language': 'fr'`\n",
    "- Curly braces hold **objects**: `'release_date_components': {'year': 1998, 'month': 4, 'day': 21}`\n",
    "- Square brackets hold **arrays**: `'featured_artists': […]`\n",
    "\n",
    "See also https://en.wikipedia.org/wiki/JSON#Syntax:\n",
    "\n",
    "- **Array**: an ordered list of zero or more elements\n",
    "- **Object**: a collection of name–value pairs where the names (also called keys) are strings. \n",
    "\n",
    "We can index this data and look at the 10th “hit” (`['hits'][9]`) about our search term \"Supreme NTM\":\n",
    "\n",
    "NB: the code `\\xa0` represents a non-breaking space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data = response.json()\n",
    "json_data['response']['hits'][9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Looping Through JSON Data\n",
    "\n",
    "We can see that each `hits` in the `response` corresponds to a song. With a `for` loop, we can easily extract the title (`full_title`) as well as the `id` of each song:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for song in json_data['response']['hits']:\n",
    "    print(\n",
    "        f\"{str(song['result']['id']):10}\", # constrain string length to align\n",
    "        song['result']['full_title']\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A closer look reveals other relevant informations:\n",
    "    \n",
    "- date of release = ???\n",
    "- number of visits to the associated page = ???\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for song in json_data['response']['hits']:\n",
    "    print(\n",
    "        f\"{str(song['result']['id']):10}\",\n",
    "        song['result']['release_date_components']['year'],\n",
    "        f\"{str(song['result']['stats']['pageviews']):8}\",\n",
    "        song['result']['full_title']\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take advantage of what we've already learned to store those metadata in a dataframe. We also take this opportunity to extract :\n",
    "\n",
    "- artist name = ???\n",
    "- artist id =  ???\n",
    "\n",
    "NB. This `artist_id` will allow us to automatically extract information about the band using the `artists` route. (`GET /artists/:id`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs_meta = []\n",
    "for song in json_data['response']['hits']:\n",
    "    songs_meta.append([song['result']['id'],\n",
    "                       song['result']['full_title'],\n",
    "                       song['result']['release_date_components']['year'],\n",
    "                       song['result']['stats']['pageviews'],\n",
    "                       song['result']['primary_artist']['id'],\n",
    "                       song['result']['artist_names']\n",
    "])\n",
    "\n",
    "#Make a Pandas dataframe from a list\n",
    "songs_df = pd.DataFrame(songs_meta)\n",
    "songs_df.columns = ['song_id', 'song_title', 'year', 'page_views', 'artist_id', 'artist_names']\n",
    "songs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_songs_meta_of_a_search(band_name):\n",
    "    genius_search_url = f\"http://api.genius.com/search?q={band_name}&access_token={api_key.your_client_access_token}\"\n",
    "    json_data = requests.get(genius_search_url).json()\n",
    "    songs_meta = []\n",
    "    for song in json_data['response']['hits']:\n",
    "        songs_meta.append([song['result']['id'],\n",
    "                           song['result']['full_title'],\n",
    "                           song['result']['release_date_components']['year'],\n",
    "                           song['result']['stats']['pageviews'],\n",
    "                           song['result']['primary_artist']['id'],\n",
    "                           song['result']['artist_names']\n",
    "                          ])\n",
    "    \n",
    "    songs_df = pd.DataFrame(songs_meta)\n",
    "    songs_df.columns = ['song_id', 'song_title', 'year', 'page_views', 'artist_id', 'artist_names']\n",
    "    return(songs_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_songs_meta_of_a_search('Orelsan')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercice 1\n",
    "\n",
    "Extract the NTM band description using the following route, which does not require an authentication token: https://genius.com/api/artists/{artist_id}\n",
    "\n",
    "- find the band ID\n",
    "- discover the [`text_format` query parameter](https://docs.genius.com/#/response-format-h1)find group identifier that can be used to specify how text content is formatted. \n",
    "\n",
    "**Answer:**\n",
    "\n",
    "```python\n",
    "artist_id = \"24568\"\n",
    "text_format = 'plain'\n",
    "\n",
    "genius_api_url = f\"https://genius.com/api/artists/{artist_id}?text_format={text_format}\"\n",
    "json_data = requests.get(genius_api_url).json()\n",
    "print(json_data['response']['artist']['description']['plain'])\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercice 2\n",
    "\n",
    "https://docs.genius.com/#artists-h2:\n",
    "\n",
    "```\n",
    "GET /artists/:id/songs\n",
    "\n",
    "Documents (songs) for the artist specified. By default, 20 items are returned for each request.\n",
    "````\n",
    "\n",
    "The query parameter `sort` sorts songs by popularity.  \n",
    "**Goal**: write a function that returns a list of the n most popular songs for an artist_id.\n",
    "\n",
    "\n",
    "**Answer**:\n",
    "\n",
    "```python\n",
    "def get_popular_songs_of_artist(artist_id, sort_param, max_number):\n",
    "    genius_api_url = f\"https://genius.com/api/artists/{artist_id}/songs?sort={sort_param}&per_page={max_number}text_format=plain\"\n",
    "    json_data = requests.get(genius_api_url).json()\n",
    "\n",
    "    for song in json_data['response']['songs']:\n",
    "        year = song['release_date_components']['year'] if song['release_date_components'] else 'none'\n",
    "        print(song['id'],\n",
    "              song['annotation_count'],\n",
    "              year,\n",
    "              song['full_title'])\n",
    "```\n",
    "\n",
    "and\n",
    "\n",
    "```python\n",
    "> get_popular_songs_of_artist('24568', 'popularity', 10)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LyricsGenius Wrapper\n",
    "\n",
    "Our initial aim (already achieved) was to extract song lyrics. But is there a simpler, more durable way? There is a `song` resource, and its [documentation](https://docs.genius.com/#songs-h2) is encouraging.\n",
    "\n",
    "> A song is a document hosted on Genius. It's usually music lyrics.\n",
    "\n",
    "Let's have a look.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import JSON\n",
    "song_id = '87367'\n",
    "json_data = requests.get(f\"https://api.genius.com/songs/{song_id}?access_token={api_key.your_client_access_token}\").json()\n",
    "JSON(json_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Disappointing!** Unfortunately, contrary to what the documentation claims, the lyrics are not accessible.  \n",
    "The Genius API appears extremely restrictive, probably for commercial reasons (it's easier to contribute than to grab the data...). In this case, it may be useful to use a wrapper.\n",
    "\n",
    "An **API wrapper** is a language-specific package or kit that encapsulates multiple API calls to make complicated functions easy to use. \n",
    "\n",
    "For Genius, there's an excellent wrapper, freely available, **LyricsGenius**. \n",
    "\n",
    "- code: https://github.com/johnwmillr/LyricsGenius\n",
    "- documentation: https://lyricsgenius.readthedocs.io/en/master/ \n",
    "\n",
    "The [method implemented to extract lyrics](https://github.com/johnwmillr/LyricsGenius/blob/master/lyricsgenius/genius.py#L95) is quite similar to ours. However, the code is of better quality: it's more tried and tested, and we can expect it to be maintained. And it's easier to use. There's no need, for example, to find the song identifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install git+https://github.com/johnwmillr/LyricsGenius.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lyricsgenius\n",
    "genius = lyricsgenius.Genius(api_key.your_client_access_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artist = genius.search_artist(\"NTM\", max_songs=3, sort=\"title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(artist, type(artist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L’objet `artist` stocke toutes les informations relatives à un artiste, notamment :\n",
    "\n",
    "- son nom: `artist.name`\n",
    "- son id: `artist.id`\n",
    "- la liste de ses chansons: `artist.songs`\n",
    "\n",
    "On peut ainsi facilement accéder à chacune des chansons pour obtenir, par exemple pour la seconde de la liste:\n",
    "\n",
    "- le titre: `artist.songs[1].title`\n",
    "- l’id: `artist.songs[1].id`\n",
    "- les paroles: `artist.songs[1].lyrics`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print and analyze all the current properties and values of `artist` object?\n",
    "'''\n",
    "from pprint import pprint\n",
    "pprint(vars(artist))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(artist.name, artist.id)\n",
    "print('=====')\n",
    "print(artist.songs[1].title, artist.songs[1].id)\n",
    "print('=====')\n",
    "print(artist.songs[1].lyrics[56:140], '…')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's easy to loop on the song list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for song in artist.songs:\n",
    "    print(song.title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LyricsGenius permet d’accéder directement aux paroles d’une chanson:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_title_search = 'Encore là'\n",
    "band_name_search = 'NTM'\n",
    "print(genius.search_song(song_title_search, band_name_search).lyrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It becomes very easy to write a small function to build a search corpus..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lyricsgenius\n",
    "import pandas as pd\n",
    "\n",
    "def get_artists_lyrics(artist_names_list, max_songs, sort_criteria):\n",
    "    \n",
    "    genius = lyricsgenius.Genius(api_key.your_client_access_token)\n",
    "    lyrics_df = pd.DataFrame(columns=['artist_name', 'artist_id', 'song_id', 'song_title', 'song_lyrics'])\n",
    "    \n",
    "    for artist_name in artist_names_list:\n",
    "        artist = genius.search_artist(artist_name, max_songs=max_songs, sort=sort_criteria)\n",
    "        artist_name = artist.name\n",
    "        artist_id = artist.id\n",
    "        songs_meta = []\n",
    "        for song in artist.songs:\n",
    "            song_meta = [\n",
    "                str(artist_name),\n",
    "                str(artist_id),\n",
    "                str(song.id),\n",
    "                str(song.title),\n",
    "                str(song.lyrics)\n",
    "            ]\n",
    "            lyrics_df.loc[len(lyrics_df)] = song_meta\n",
    "    return(lyrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_df = get_artists_lyrics(['NTM', 'Orelsan'], 3, 'popularity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The corpus is rapidly built up for analysis: stylometry, attribution or topic modeling, etc.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lyrics_df.iloc[1].song_lyrics[:240])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Issue**. We see that the first line contains credits... `.index()` allows us to position ourselves after the first line break."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# problème on constate que la première ligne contient des crédits… la méthode .index() permet de se positionner après le premier saut de ligne.\n",
    "lyrics = lyrics_df.iloc[1].song_lyrics[:240]\n",
    "print(lyrics[lyrics.index('\\n')+1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can apply this method to the `song_lyrics` column of the dataframe, in order to improve the data. Alternatively, we could rewrite our `get_artists_lyrics()` function.\n",
    "**Warning**. Be careful to apply only once, otherwise the first line of the lyrics will be lost each time it is run..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_df['song_lyrics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_df['song_lyrics'] = lyrics_df['song_lyrics'].apply(lambda x: x[x.index('\\n')+1:])\n",
    "lyrics_df['song_lyrics']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Genius, we discovered JSON and learned how to browse it to extract data. We read its API documentation and managed a token for authentication. Finally, we discovered a first wrapper and its documentation, so we could easily build up a search corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gallica Search API (XML)\n",
    "\n",
    "[Gallica](https://gallica.bnf.fr/) is the digital library of the Bibliothèque nationale de France and its partners. It has been freely accessible since 1997, and contains several million documents (>10).\n",
    "\n",
    "Above all, BnF has done an impressive job of making these open resources available, via various APIs that enable interaction with different services: metadata, textual data, but also iconographic data recollection.\n",
    "\n",
    "These [Gallica's APIs are documented](https://api.bnf.fr/recherche?f[0]=sources:197) in exemplary manner, with particular attention paid to the needs of researchers.\n",
    "\n",
    "**Using the Search API, let's start by trying to automatically collect data relating to Jules Verne's novels.**  \n",
    "\n",
    "**This API allows you to search the Gallica digital collection:**\n",
    "\n",
    "- **Documentation**: [https://api.bnf.fr/fr/api-gallica-de-recherche](https://api.bnf.fr/fr/api-gallica-de-recherche)\n",
    "- **SRU Gallica search service**: https://api.bnf.fr/fr/api-gallica-de-recherche#scroll-nav__3\n",
    "\n",
    "SRU (Search/Retrieval via URL) is a **standard metadata exchange protocol** adapted to the needs of library catalogues. In other words, the libraries have agreed to define a standardised way of offering their catalogue as a web service.\n",
    "\n",
    "And the SRU standard specifies **how to query** the server that exposes the catalogue data and **how to format the response**.\n",
    "\n",
    "Read: https://bibliotheques.wordpress.com/2017/10/27/papa-cest-quoi-un-sru/\n",
    "\n",
    "![SRU](img/SRU.png)\n",
    "\n",
    "\n",
    "#### CQL Query\n",
    "\n",
    "The basics: https://gallica.bnf.fr/SRU?version=1.2&operation=searchRetrieve&query={CQL_QUERY}\n",
    "\n",
    "\n",
    "[CQL, the Contextual Query Language](https://www.loc.gov/standards/sru/cql/), is a formal language for representing queries to information retrieval systems such as web indexes, bibliographic catalogs and museum collection information. The design objective is that queries be human readable and writable, and that the language be intuitive while maintaining the expressiveness of more complex languages.\n",
    "\n",
    "\n",
    "A few examples to help you understand.\n",
    "\n",
    "\n",
    "**Documents mentioning \"Jules Verne\" in Gallica** (>18752 records…):\n",
    "\n",
    "- `query=gallica all \"Jules Verne\"`\n",
    "- [https://gallica.bnf.fr/SRU?version=1.2&operation=searchRetrieve&**query=gallica all \"Jules Verne\"**](https://gallica.bnf.fr/SRU?version=1.2&operation=searchRetrieve&query=gallica%20all%20%22Jules%20Verne%22)\n",
    "\n",
    "**Documents of which \"Jules Verne\" is the author in Gallica** (>400 records):  \n",
    "\n",
    "- `query=dc.creator all \"jules verne\"`\n",
    "- [https://gallica.bnf.fr/SRU?version=1.2&operation=searchRetrieve&**query=dc.creator all \"Jules Verne\"**](https://gallica.bnf.fr/SRU?version=1.2&operation=searchRetrieve&query=dc.creator%20all%20%22Jules%20Verne%22)\n",
    "\n",
    "**Books (monographs) in French authored by \"Jules Verne\" in Gallica** (>290 records):\n",
    "\n",
    "- `query=dc.creator all \"jules verne\"&filter=dc.type all \"monographie\" and dc.language all \"fre\"`\n",
    "- [https://gallica.bnf.fr/SRU?version=1.2&operation=searchRetrieve&**query=dc.creator all \"jules verne\"&filter=dc.type all \"monographie\" and dc.language all \"fre\"**](https://gallica.bnf.fr/SRU?version=1.2&operation=searchRetrieve&query=dc.creator%20all%20%22jules%20verne%22&filter=dc.type%20all%20%22monographie%22%20and%20dc.language%20all%20%22fre%22)\n",
    "\n",
    "\n",
    "There are also **sorting criteria**. Results can be sorted according to OCR quality: `ocr.quality/sort.descending`:\n",
    "\n",
    "[https://gallica.bnf.fr/SRU?version=1.2&operation=searchRetrieve&query=dc.creator all \"jules verne\" sortby ocr.quality/sort.descending&filter=dc.type all \"monographie\" and dc.language all \"fre\"](https://gallica.bnf.fr/SRU?version=1.2&operation=searchRetrieve&query=dc.creator%20all%20%22jules%20verne%22%20sortby%20ocr.quality/sort.descending&filter=dc.type%20all%20%22monographie%22%20and%20dc.language%20all%20%22fre%22)\n",
    "\n",
    "There are other parameters:\n",
    " \n",
    "- `startRecord`: the pagination system index, between 1 and the maximum number of results returned by the query\n",
    "- `maximumRecords`: the number of results returned by the service (from 0 to a maximum of 50). By default, if this parameter is not present, the value is 15.\n",
    "\n",
    "\n",
    "**The 5 Jules Verne books with the best OCR quality**:\n",
    "\n",
    "- `query=dc.creator all \"jules verne\" sortby ocr.quality/sort.descending&filter=dc.type all \"monographie\" and dc.language all \"fre\"&maximumRecords=5`\n",
    "- [https://gallica.bnf.fr/SRU?version=1.2&operation=searchRetrieve&query=dc.creator all \"jules verne\" sortby ocr.quality/sort.descending&filter=dc.type all \"monographie\" and dc.language all \"fre\"&maximumRecords=5](https://gallica.bnf.fr/SRU?version=1.2&operation=searchRetrieve&query=dc.creator%20all%20%22jules%20verne%22%20sortby%20ocr.quality/sort.descending&filter=dc.type%20all%20%22monographie%22%20and%20dc.language%20all%20%22fre%22&maximumRecords=5)\n",
    "\n",
    "We're going to start working on this very small corpus to understand the format of the response.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "url = \"https://gallica.bnf.fr/SRU?version=1.2&operation=searchRetrieve&query=dc.creator%20all%20%22jules%20verne%22%20sortby%20ocr.quality/sort.descending&filter=dc.type%20all%20%22monographie%22%20and%20dc.language%20all%20%22fre%22&maximumRecords=5\"\n",
    "response = requests.get(url)\n",
    "response.headers['Content-Type']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, the response is not formatted as JSON, but as XML.  \n",
    "Unfortunately, `Requests` does not handle parsing XML responses… :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can observe the XML structure of a notice.\n",
    "\n",
    "```xml\n",
    "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"yes\"?>\n",
    "<srw:searchRetrieveResponse\n",
    "    xmlns:srw=\"http://www.loc.gov/zing/srw/\"\n",
    "    xmlns:oai_dc=\"http://www.openarchives.org/OAI/2.0/oai_dc/\"\n",
    "    xmlns:dc=\"http://purl.org/dc/elements/1.1/\">\n",
    "    <srw:records>\n",
    "        <srw:record>\n",
    "            <srw:recordSchema>http://www.openarchives.org/OAI/2.0/OAIdc.xsd</srw:recordSchema>\n",
    "            …\n",
    "            <srw:recordData>\n",
    "                <oai_dc:dc>\n",
    "                    <dc:creator>Verne, Jules (1828-1905)…</dc:creator>\n",
    "                    <dc:date>1896</dc:date>\n",
    "                    <dc:identifier>https://gallica.bnf.fr/ark:/12148/bpt6k65501998</dc:identifier>\n",
    "                    <dc:language>fre</dc:language>\n",
    "                    <dc:source>Bibliothèque nationale de France…</dc:source>\n",
    "                    <dc:title>Les voyages du Capitaine Cook…</dc:title>\n",
    "                    <dc:identifier>https://gallica.bnf.fr/ark:/12148/bpt6k65501998</dc:identifier>\n",
    "                </oai_dc:dc>\n",
    "            </srw:recordData>\n",
    "            …\n",
    "        </srw:record>\n",
    "        <srw:record/>\n",
    "        …\n",
    "    </srw:records>\n",
    "</srw:searchRetrieveResponse>\n",
    "```\n",
    "\n",
    "To extract the information… **we need to parse XML data**!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XML Parsing\n",
    "\n",
    "**[The ElementTree XML API](https://docs.python.org/3/library/xml.etree.elementtree.html)**. The `xml.etree.ElementTree` (`ET` in short) module implements a simple and efficient API for parsing and creating XML data.\n",
    "\n",
    "XML is an inherently hierarchical data format, and the most natural way to represent it is with a tree. **ET has two classes** for this purpose:\n",
    "\n",
    "- `ElementTree` represents the whole XML document as a tree\n",
    "- `Element` represents a single node in this tree.\n",
    "\n",
    "Interactions with the whole document (reading and writing to/from files) are usually done on the ElementTree level. Interactions with a single XML element and its sub-elements are done on the Element level.\n",
    "\n",
    "\n",
    "**[fromstring() parses XML from a string directly into an Element](https://docs.python.org/3/library/xml.etree.elementtree.html#xml.etree.ElementTree.fromstring)** –stored below in the `root` variable– which is the root element of the parsed tree.\n",
    "\n",
    "Then, `Element` has some useful methods that help iterate recursively over all the sub-tree below it (its children, their children, and so on). For example, `Element.iter()`.\n",
    "\n",
    "**[The iter() method](https://docs.python.org/3/library/xml.etree.elementtree.html#xml.etree.ElementTree.Element.iter)** creates a tree iterator with the current element as the root. The iterator iterates over this element and all elements below it, in document (depth first) order. If tag is not None or '*', only elements whose tag equals tag are returned from the iterator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "root = ET.fromstring(response.content)\n",
    "for child in root.iter('*'):\n",
    "    print(child.tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metadata extraction\n",
    "\n",
    "We can see that all the elements of the response are accessible, according to their namespace: `{http://purl.org/dc/elements/1.1/}creator`  \n",
    "The `creator` element is declared for the Dublin Core namespace (`http://purl.org/dc/elements/1.1/`).\n",
    "\n",
    "Using a `for` loop, each `record` is read, and the `Element.findall()` method is used to extract Dublin Core metadata:\n",
    "\n",
    "- `identifier`\n",
    "- `date`\n",
    "- `title`\n",
    "- `creator`\n",
    "\n",
    "NB. The [`Element.findall()` method](https://docs.python.org/3/library/xml.etree.elementtree.html#xml.etree.ElementTree.Element.findall) finds only elements with a tag which are direct children of the current element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "root = ET.fromstring(response.content)\n",
    "for record in root.iter('{http://www.loc.gov/zing/srw/}record'):\n",
    "    dc_identifier = record.findall('{http://www.loc.gov/zing/srw/}recordData/{http://www.openarchives.org/OAI/2.0/oai_dc/}dc/{http://purl.org/dc/elements/1.1/}identifier')[0].text\n",
    "    dc_date = record.findall('{http://www.loc.gov/zing/srw/}recordData/{http://www.openarchives.org/OAI/2.0/oai_dc/}dc/{http://purl.org/dc/elements/1.1/}date')[0].text\n",
    "    dc_title = record.findall('{http://www.loc.gov/zing/srw/}recordData/{http://www.openarchives.org/OAI/2.0/oai_dc/}dc/{http://purl.org/dc/elements/1.1/}title')[0].text\n",
    "    dc_creator = record.findall('{http://www.loc.gov/zing/srw/}recordData/{http://www.openarchives.org/OAI/2.0/oai_dc/}dc/{http://purl.org/dc/elements/1.1/}creator')[0].text\n",
    "    print(dc_creator, dc_identifier, dc_date, dc_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same thing, but loading data into a dataframe\n",
    "import pandas as pd\n",
    "metadata = []\n",
    "root = ET.fromstring(response.content)\n",
    "for record in root.iter('{http://www.loc.gov/zing/srw/}record'):\n",
    "    dc_identifier = record.findall('{http://www.loc.gov/zing/srw/}recordData/{http://www.openarchives.org/OAI/2.0/oai_dc/}dc/{http://purl.org/dc/elements/1.1/}identifier')[0].text\n",
    "    dc_date = record.findall('{http://www.loc.gov/zing/srw/}recordData/{http://www.openarchives.org/OAI/2.0/oai_dc/}dc/{http://purl.org/dc/elements/1.1/}date')[0].text\n",
    "    dc_title = record.findall('{http://www.loc.gov/zing/srw/}recordData/{http://www.openarchives.org/OAI/2.0/oai_dc/}dc/{http://purl.org/dc/elements/1.1/}title')[0].text\n",
    "    dc_creator = record.findall('{http://www.loc.gov/zing/srw/}recordData/{http://www.openarchives.org/OAI/2.0/oai_dc/}dc/{http://purl.org/dc/elements/1.1/}creator')[0].text\n",
    "    ocr_quality = record.findall('{http://www.loc.gov/zing/srw/}extraRecordData/nqamoyen')[0].text\n",
    "    metadata.append({\n",
    "        'ocr_quality': ocr_quality,\n",
    "        'dc_creator': dc_creator,\n",
    "        'dc_identifier': dc_identifier,\n",
    "        'dc_date': dc_date,\n",
    "        'dc_title': dc_title\n",
    "    })\n",
    "metadata_df = pd.DataFrame(metadata)\n",
    "metadata_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We make the code reusable by defining a function.\n",
    "def get_gallica_metadata(search_api_url):\n",
    "    response = requests.get(url)\n",
    "    root = ET.fromstring(response.content)\n",
    "    metadata = []\n",
    "    for record in root.iter('{http://www.loc.gov/zing/srw/}record'):\n",
    "        dc_identifier = record.findall('{http://www.loc.gov/zing/srw/}recordData/{http://www.openarchives.org/OAI/2.0/oai_dc/}dc/{http://purl.org/dc/elements/1.1/}identifier')[0].text\n",
    "        dc_date = record.findall('{http://www.loc.gov/zing/srw/}recordData/{http://www.openarchives.org/OAI/2.0/oai_dc/}dc/{http://purl.org/dc/elements/1.1/}date')[0].text if record.findall('{http://www.loc.gov/zing/srw/}recordData/{http://www.openarchives.org/OAI/2.0/oai_dc/}dc/{http://purl.org/dc/elements/1.1/}date') else 'NaN'\n",
    "        dc_title = record.findall('{http://www.loc.gov/zing/srw/}recordData/{http://www.openarchives.org/OAI/2.0/oai_dc/}dc/{http://purl.org/dc/elements/1.1/}title')[0].text\n",
    "        dc_creator = record.findall('{http://www.loc.gov/zing/srw/}recordData/{http://www.openarchives.org/OAI/2.0/oai_dc/}dc/{http://purl.org/dc/elements/1.1/}creator')[0].text\n",
    "        ocr_quality = record.findall('{http://www.loc.gov/zing/srw/}extraRecordData/nqamoyen')[0].text\n",
    "        metadata.append({\n",
    "            'ocr_quality': ocr_quality,\n",
    "            'dc_creator': dc_creator,\n",
    "            'dc_identifier': dc_identifier,\n",
    "            'dc_date': dc_date,\n",
    "            'dc_title': dc_title\n",
    "        })\n",
    "    metadata_df = pd.DataFrame(metadata)\n",
    "    return metadata_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same thing. But this version of the function, which is a little more abstract, allows you to specify the list of fields to be extracted as an argument (`dc_elements_array`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same. Pass the list of fields to be extracted\n",
    "def get_gallica_metadata(search_api_url, dc_elements_array):\n",
    "    response = requests.get(url)\n",
    "    root = ET.fromstring(response.content)\n",
    "    metadata = []\n",
    "    for record in root.iter('{http://www.loc.gov/zing/srw/}record'):\n",
    "        record_meta_dic = {}\n",
    "        for dc_el in dc_elements_array:\n",
    "            if record.findall('{http://www.loc.gov/zing/srw/}recordData/{http://www.openarchives.org/OAI/2.0/oai_dc/}dc/{http://purl.org/dc/elements/1.1/}'+dc_el)[0].text:\n",
    "                record_meta_dic[dc_el] = record.findall('{http://www.loc.gov/zing/srw/}recordData/{http://www.openarchives.org/OAI/2.0/oai_dc/}dc/{http://purl.org/dc/elements/1.1/}'+dc_el)[0].text\n",
    "            else :\n",
    "                record_meta_dic[dc_el] = 'NaN'\n",
    "        # non DC metadata\n",
    "        record_meta_dic['ocr_quality'] = record.findall('{http://www.loc.gov/zing/srw/}extraRecordData/nqamoyen')[0].text if record.findall('{http://www.loc.gov/zing/srw/}extraRecordData/nqamoyen')[0].text else 'NaN'\n",
    "        metadata.append(record_meta_dic)\n",
    "    metadata_df = pd.DataFrame(metadata)\n",
    "    return metadata_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://gallica.bnf.fr/SRU?version=1.2&operation=searchRetrieve&query=dc.creator%20all%20%22jules%20verne%22%20sortby%20ocr.quality/sort.descending&filter=dc.type%20all%20%22monographie%22%20and%20dc.language%20all%20%22fre%22&startRecord=1&maximumRecords=5\"\n",
    "books_df = get_gallica_metadata(url, ['creator', 'identifier', 'title'])\n",
    "books_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save DF to CSV\n",
    "\n",
    "The Pandas [pd.DataFrame.to_csv](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_csv.html#pandas-dataframe-to-csv) method exports the dataframe object to a csv file.\n",
    "\n",
    "Several parameters are useful here:\n",
    "\n",
    "- `sep`: field delimiter for the output file.\n",
    "- `index`: write or not row names (index).\n",
    "- `encoding`: the encoding to use in the output file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df.to_csv('./output/books.tsv', sep='\\t', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also specify the list of columns to be exported:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df.to_csv('./output/books.tsv', sep='\\t', encoding='utf-8', index=False,\n",
    "               columns=['creator', 'identifier', 'title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the start of the course, we inherited a TSV file for accessing the Gutenberg versions of Jules Verne's novels. This time, thanks to Gallica's Search API and your new skills in XML data parsing, you've automatically built up this table yourself.\n",
    "\n",
    "**Now it's time to retrieve the text too!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gallica Document API\n",
    "\n",
    "Documentation: https://api.bnf.fr/fr/api-document-de-gallica\n",
    "\n",
    "From a document found via the Search API or the Gallica interface, the Document API can be used to retrieve the metadata needed to use the document's digital resources, including:\n",
    "\n",
    "- bibliographic informations\n",
    "- search hits\n",
    "- text (plain text / OCR)\n",
    "\n",
    "Thus, for an ark identifier, it is always possible to retrieve the metadata of the OAI record:\n",
    "\n",
    "https://gallica.bnf.fr/services/OAIRecord?ark={ark}\n",
    "\n",
    "This service returns the document's OAI-PMH record as well as other technical information, such as document type, or whether or not full-text searching is available.\n",
    "\n",
    "Only one parameter is mandatory: the ark of the document's numerical identifier.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Metadata retrieval\n",
    "\n",
    "If necessary, we can find all the metadata we need for our edition of *Les voyages du Capitaine Cook*:  \n",
    "https://gallica.bnf.fr/services/OAIRecord?ark=ark:/12148/bpt6k65501998\n",
    "\n",
    "We need to get its ark in our dataframe `books_df`:\n",
    "\n",
    "a. Find the corresponding line (= the line whose 'title' cell contains 'Captain Cook'):  \n",
    "`books_df.loc[books_df['title'].str.contains('Capitaine Cook')]`  \n",
    "Note that str.contains() is case sensitive.\n",
    "\n",
    "b. Extract the identifier (https://gallica.bnf.fr/ark:/12148/bpt6k6550199`):  \n",
    "`books_df.loc[books_df['title'].str.contains('Capitaine Cook')]['identifier'][1]`  \n",
    "NB. the final `[1]` index is necessary because [`.loc`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.loc.html) returns a list.\n",
    "\n",
    "c. Extract its ark ('ark:/12148/bpt6k65501998'), substring of the identifier:  \n",
    "`books_df.loc[books_df['title'].str.contains('Capitaine Cook')]['identifier'][1].split('https://gallica.bnf.fr/')[1]`  \n",
    "or, more simply, we use the string index:  \n",
    "`books_df.loc[books_df['title'].str.contains('Capitaine Cook')]['identifier'][1][23:]`\n",
    "\n",
    "=====\n",
    "Or, even simpler, you already know the ark (and that's it!):  \n",
    "`ark:/12148/bpt6k65501998`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_ark = books_df.loc[books_df['title'].str.contains('Capitaine Cook')]['identifier'][1][23:]\n",
    "my_novel_record = requests.get('https://gallica.bnf.fr/services/OAIRecord?ark='+my_ark)\n",
    "print(my_novel_record.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here, you can put your new skills in XML parsing to good use, discovering, for example, the OCR qulity available for this book:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xml.etree import ElementTree\n",
    "tree = ElementTree.fromstring(my_novel_record.content)\n",
    "tree.find('nqamoyen').text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or to find related resources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for relation in tree.iter('{http://purl.org/dc/elements/1.1/}relation'):\n",
    "    print(relation.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text retrieval (plain text)\n",
    "\n",
    "It's really very easy!\n",
    "\n",
    "When a document is indexed in full text, it is possible to obtain this text, using the `textBrut` qualifier:  \n",
    "https://gallica.bnf.fr/{ark}.texteBrut\n",
    "\n",
    "Let's retrieve the text from *Les voyages du Capitaine Cook*:  \n",
    "https://gallica.bnf.fr/ark:/12148/bpt6k65501998.texteBrut\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_url = f\"https://gallica.bnf.fr/{my_ark}.texteBrut\"\n",
    "response = requests.get(text_url)\n",
    "response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do we find? Surprisingly, the text is not actually available in plain text, but is formatted in HTML…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.headers['Content-Type']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But that's a good thing! We're going to be able to put to good use the BeautifulSoup skills we've acquired in processing the files shared by Project Gutenberg, so as to extract Jules Verne's text alone.\n",
    "\n",
    "You need to analyze the HTML publication model, so as to cut out the metadata at the beginning of the file (pay attention to the use of the `hr` element...). Test before wrapping the code in a small function `get_book_text()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "document = BeautifulSoup(response.text, \"html.parser\")\n",
    "first_hr_tag = document.select('hr')[0]\n",
    "text = ''\n",
    "for p in first_hr_tag.find_all_next('p'):\n",
    "    text += p.text + '\\n'\n",
    "#print(text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_book_text(ark_url):\n",
    "    response = requests.get(ark_url+'.texteBrut')\n",
    "    document = BeautifulSoup(response.text, \"html.parser\")\n",
    "    first_hr_tag = document.select('hr')[0]\n",
    "    text = ''\n",
    "    for p in first_hr_tag.find_all_next('p'):\n",
    "        text += p.text + '\\n'\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we just need to retrieve the text for each of our novels, by applying our little function to each line of our dataframe...\n",
    "\n",
    "**That's easy! We simply add a 'text' column.  \n",
    "But it's a bit time-consuming, because you have to retrieve all the data via HTTP, then process it... Don't launch the cell until you're sure!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df['text'] = books_df.apply(lambda x: get_book_text(x['identifier']), axis=1)\n",
    "books_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(books_df.iloc[1]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you can also specify the pages you wish to retrieve, using the `f[X]n[y]`qualifier (X is the page number of the start page, and n is the number of subsequent pages):\n",
    "\n",
    "https://gallica.bnf.fr/ark:/12148/{ark}/f{start_page_number}n{number_of_pages}.texteBrut\n",
    "\n",
    "For example, the first 5 pages of the chapter devoted to Bougainville (p. 75-79 => 5 pages from f87)\n",
    "\n",
    "https://gallica.bnf.fr/ark:/12148/bpt6k65501998/f87n5.texteBrut\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Text to File\n",
    "\n",
    "We often need to save data locally..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = books_df.iloc[1]['identifier'][34:]\n",
    "with open(f'output/{file_name}.txt', 'a') as file:\n",
    "    file.write(books_df.iloc[1]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With [.iterrows()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.iterrows.html), you can iterate over DataFrame rows for easy export each text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in books_df.iterrows():\n",
    "    file_name = row['identifier'][34:]\n",
    "    with open(f'output/{file_name}.txt', 'w') as file:\n",
    "        file.write(row['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may also need to save the text retrieved via the API directly to a file.  \n",
    "But remember: the text is actually formatted in HTML and we'll need to process it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_ark = 'ark:/12148/bpt6k65501998'\n",
    "response_text = requests.get(f\"https://gallica.bnf.fr/{my_ark}.texteBrut\").text\n",
    "with open(f'./output/{my_ark[11:]}.html', 'w') as file:\n",
    "    file.write(response_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text retrieval (OCR)\n",
    "\n",
    "The Gallica API can be used to extract an OCR page from the digital library:\n",
    "\n",
    "https://gallica.bnf.fr/RequestDigitalElement?O={ark}&E=ALTO&Deb={page_number}\n",
    "\n",
    "For example, the ALTO on the first page of the first chapter devoted to Bougainville (p. 1 => f13)\n",
    "\n",
    "https://gallica.bnf.fr/RequestDigitalElement?O=bpt6k65501998&E=ALTO&Deb=13\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(requests.get('https://gallica.bnf.fr/RequestDigitalElement?O=bpt6k65501998&E=ALTO&Deb=13').text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's XML, ALTO to be precise.\n",
    "\n",
    "[ALTO](https://en.wikipedia.org/wiki/ALTO_(XML)) (Analysed Layout and Text Object) is an XML standard for reporting the physical layout and logical structure of text transcribed by optical character recognition (OCR).\n",
    "\n",
    "It contains for each text box:\n",
    "\n",
    "- the text\n",
    "- its coordinates\n",
    "- the recognition confidence rate\n",
    "- and even formatting elements\n",
    "\n",
    "The format allows images and text to be superimposed (PDF like).\n",
    "\n",
    "We don't have time to go into detail about the potential of the ALTO. Let's take a simple example that allows us to put text to one side: let's look at images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks to [Gallica's IIIF API](https://api.bnf.fr/fr/api-iiif-de-recuperation-des-images-de-gallica), you can also display the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "page_iiif_url = 'https://gallica.bnf.fr/iiif/ark:/12148/bpt6k65501998/f13/full/400,/0/native.jpg'\n",
    "Image(url=page_iiif_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The illustration is tagged in the ALTO (element `Illustration`) and we can extract its identifier, as well as its coordinates in the page:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xml.etree import ElementTree\n",
    "tree = ElementTree.fromstring(requests.get('https://gallica.bnf.fr/RequestDigitalElement?O=bpt6k65501998&E=ALTO&Deb=13').content)\n",
    "for illustration in tree.iter('{http://bibnum.bnf.fr/ns/alto_prod}Illustration'):\n",
    "    print(illustration.get('ID'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "illustrations = []\n",
    "for illustration in tree.iter('{http://bibnum.bnf.fr/ns/alto_prod}Illustration'):\n",
    "    illustrations.append({\n",
    "            'ID': illustration.get('ID'),\n",
    "            'HPOS': illustration.get('HPOS'),\n",
    "            'VPOS': illustration.get('VPOS'),\n",
    "            'WIDTH': illustration.get('WIDTH'),\n",
    "            'HEIGHT': illustration.get('HEIGHT')\n",
    "        })\n",
    "illustrations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks to the IIIF API, we can easily display (or extract for analysis) this single illustration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "illustration_iiif_url = ('https://gallica.bnf.fr/iiif/ark:/12148/bpt6k65501998/f13/'\n",
    "      + illustrations[0]['HPOS'] +','\n",
    "      + illustrations[0]['VPOS'] +','\n",
    "      + illustrations[0]['WIDTH'] +','\n",
    "      + illustrations[0]['HEIGHT']\n",
    "      + '/full/0/native.jpg')\n",
    "\n",
    "Image(url=illustration_iiif_url, width=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrappers\n",
    "\n",
    "- PyGallica : https://github.com/ian-nai/PyGallica\n",
    "- gallipy : https://libraries.io/pypi/gallipy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "Let's mobilize the spaCy skills we've acquired to write a pre-processing function that will normalize all tokens and exclude stopwords.\n",
    "\n",
    "With [.copy()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.copy.html), we create a new object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df_prep = books_df.copy()\n",
    "books_df_prep.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing functions can be defined below (see spaCy introduction), here :\n",
    "\n",
    "- Lowercases the text\n",
    "- Lemmatizes each token\n",
    "- Removes punctuation symbols\n",
    "- Removes stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing functions\n",
    "import spacy\n",
    "from spacy.lang.fr.stop_words import STOP_WORDS as fr_stop\n",
    "\n",
    "import fr_core_news_md\n",
    "nlp = fr_core_news_md.load()\n",
    "\n",
    "def preprocess_lemma(token):\n",
    "    return token.lemma_.strip().lower()\n",
    "\n",
    "# Filter: a function that returns True or False for a token according to certain criteria\n",
    "def is_token_allowed(token):\n",
    "    return bool(\n",
    "        token\n",
    "        and str(token).strip()\n",
    "        and not token.is_stop\n",
    "        and not token.is_punct\n",
    "    )\n",
    "\n",
    "def preprocess_text(text):\n",
    "    doc = nlp(text)\n",
    "    filtered_doc_lemmas = [\n",
    "        preprocess_lemma(token)\n",
    "        for token in doc\n",
    "        if is_token_allowed(token)\n",
    "    ]\n",
    "    return ' '.join(filtered_doc_lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test!\n",
    "preprocess_text(books_df_prep.iloc[1]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the preprocessing function `preprocess_text()` has been tested, we can apply it with .apply() to the text of each novel contained in our dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df_prep['keywords'] = books_df_prep.apply(lambda x: preprocess_text(x['text']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df_prep.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df_prep.iloc[1]['keywords']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily adapt the code, for example to retain only stop words, if our aim is to do automatic author attribution.\n",
    "\n",
    "We have to redefine the preprocess_text() function identically, as the is_token_allowed() function it calls has been redefined to retain only stop words. But if you look closely, the modification is very slight indeed: only the is_token_allowed() function has been modified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter: a function that returns True or False for a token according to certain criteria\n",
    "def is_token_allowed(token):\n",
    "    return bool(\n",
    "        token.is_stop\n",
    "        and str(token).strip()\n",
    "    )\n",
    "\n",
    "def preprocess_text(text):\n",
    "    doc = nlp(text)\n",
    "    filtered_doc_lemmas = [\n",
    "        preprocess_lemma(token)\n",
    "        for token in doc\n",
    "        if is_token_allowed(token)\n",
    "    ]\n",
    "    return ' '.join(filtered_doc_lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test!\n",
    "preprocess_text(books_df_prep.iloc[1]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df_prep['stopwords'] = books_df_prep.apply(lambda x: preprocess_text(x['text']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df_prep.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix. Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all imports\n",
    "'''\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import display, HTML, Image, JSON\n",
    "import xml.etree.ElementTree as ET\n",
    "from xml.etree import ElementTree\n",
    "import lyricsgenius\n",
    "import spacy\n",
    "from spacy.lang.fr.stop_words import STOP_WORDS as fr_stop\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
